{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k:\\\\Bone-Marrow-Cells-Classification-CNN\\\\reasearch'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k:\\\\Bone-Marrow-Cells-Classification-CNN'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_URL: str\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "    train_data_dir: Path\n",
    "    val_data_dir: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Cnn_Classifier.constant import *\n",
    "\n",
    "\n",
    "from Cnn_Classifier.utils.common import read_yaml , create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "        create_directories([config.root_dir])\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_URL=config.source_URL,\n",
    "            local_data_file=config.local_data_file,\n",
    "            unzip_dir=config.unzip_dir,\n",
    "            train_data_dir=config.train_data_dir,\n",
    "            val_data_dir=config.val_data_dir\n",
    "        )\n",
    "        return data_ingestion_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "from Cnn_Classifier import logger\n",
    "from Cnn_Classifier.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle.json moved to C:\\Users\\Samar mubark/.kaggle\\kaggle.json with proper permissions set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the uploaded kaggle.json file\n",
    "uploaded_kaggle_json_path = 'kaggle.json'\n",
    "\n",
    "# Create the .kaggle directory in the home folder, if it doesn't exist\n",
    "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "# Move the kaggle.json to the .kaggle directory\n",
    "kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "shutil.move(uploaded_kaggle_json_path, kaggle_json_path)\n",
    "\n",
    "# Set the required permission for the file\n",
    "os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "print(f\"kaggle.json moved to {kaggle_json_path} with proper permissions set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "from pathlib import Path\n",
    "from Cnn_Classifier import logger\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        self.root_dir_path = Path(self.config.root_dir)  # Converted Path object\n",
    "        self.train_dir = self.root_dir_path / 'train_data_set'\n",
    "        self.val_dir = self.root_dir_path / 'validation_data_set'\n",
    "        self.split_ratio = 0.3\n",
    "\n",
    "    def remove_readonly_and_delete(self, file_path):\n",
    "        try:\n",
    "            os.chmod(file_path, 0o777)  # Change to allow all operations\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing file {file_path}: {e}\")\n",
    "\n",
    "    def handle_directory(self, directory_path):\n",
    "        if os.path.exists(directory_path):\n",
    "            # Change permissions and remove files in the directory\n",
    "            for filename in os.listdir(directory_path):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    self.remove_readonly_and_delete(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    self.handle_directory(file_path)  # Recursively handle subdirectories\n",
    "            os.rmdir(directory_path)  # Now remove directory\n",
    "\n",
    "\n",
    "    def download_file(self):\n",
    "        \"\"\"\n",
    "        Download data from Kaggle only if it hasn't been downloaded already.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if the dataset file already exists\n",
    "            if os.path.exists(self.config.local_data_file):\n",
    "                logger.info(f\"Dataset already downloaded: {self.config.local_data_file}\")\n",
    "                return\n",
    "\n",
    "            # Ensure Kaggle directory and credentials file exist\n",
    "            assert os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')), \"Kaggle API credentials not found\"\n",
    "\n",
    "            # Kaggle dataset command\n",
    "            dataset_command = ['kaggle', 'datasets', 'download', '-d', self.config.source_URL, '-p', str(self.root_dir_path)]\n",
    "            subprocess.run(dataset_command, check=True, capture_output=True)\n",
    "\n",
    "            # Find downloaded zip file\n",
    "            zip_files = list(self.root_dir_path.glob('*.zip'))\n",
    "            if zip_files:\n",
    "                downloaded_zip = zip_files[0]\n",
    "                shutil.move(str(downloaded_zip), str(self.config.local_data_file))\n",
    "                logger.info(f\"Downloaded data from Kaggle into file {self.config.local_data_file}\")\n",
    "            else:\n",
    "                logger.error(\"No zip file downloaded from Kaggle\")\n",
    "                raise FileNotFoundError(\"Downloaded Kaggle dataset not found\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading file: {e}\")\n",
    "            raise e\n",
    "     \n",
    "    \n",
    "   \n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        unzip_path = Path(self.config.unzip_dir)\n",
    "        os.makedirs(unzip_path, exist_ok=True)\n",
    "\n",
    "        # Check if extraction is needed by verifying if expected content exists\n",
    "        # Replace 'expected_content_folder' with an actual folder/file name you expect to find after extraction\n",
    "        expected_content_path = unzip_path / \"artifacts/data_ingestion/bone_marrow_cell_dataset\"\n",
    "\n",
    "        if expected_content_path.exists():\n",
    "            logger.info(f\"Data already extracted in {unzip_path}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_path)\n",
    "                logger.info(f\"Extracted data into {unzip_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting zip file: {e}\")\n",
    "            raise e\n",
    "\n",
    "    # ... other methods ...\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    def organize_dataset(self):\n",
    "        base_path = 'artifacts/data_ingestion/bone_marrow_cell_dataset'  # This can be set as an attribute if it's a fixed value\n",
    "        for folder in os.listdir(base_path):\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for subfolder in os.listdir(folder_path):\n",
    "                    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                    if os.path.isdir(subfolder_path):\n",
    "                        # Move files from sub-subfolders to main subfolder\n",
    "                        for file in os.listdir(subfolder_path):\n",
    "                            shutil.move(os.path.join(subfolder_path, file), folder_path)\n",
    "                        # Remove the now empty sub-subfolder\n",
    "                        os.rmdir(subfolder_path)\n",
    "\n",
    "    def augment_image(self, image_path, save_path, num_required):\n",
    "        image = Image.open(image_path)\n",
    "        augmentations = [Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270]\n",
    "        while num_required > 0:\n",
    "            aug_image = image.transpose(random.choice(augmentations))\n",
    "            aug_image.save(os.path.join(save_path, f'aug_{num_required}.jpg'))\n",
    "            num_required -= 1\n",
    "\n",
    "    def balance_dataset(self, target_count=700):\n",
    "        base_path = 'artifacts/data_ingestion/bone_marrow_cell_dataset'  # This can be set as an attribute if it's a fixed value\n",
    "        for folder in os.listdir(base_path):\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                images = [img for img in os.listdir(folder_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                current_count = len(images)\n",
    "\n",
    "                if current_count < target_count:\n",
    "                    # Augment images\n",
    "                    for i in range(target_count - current_count):\n",
    "                        image_path = os.path.join(folder_path, random.choice(images))\n",
    "                        self.augment_image(image_path, folder_path, target_count - current_count)\n",
    "\n",
    "                elif current_count > target_count:\n",
    "                    # Randomly delete images\n",
    "                    images_to_delete = random.sample(images, current_count - target_count)\n",
    "                    for img in images_to_delete:\n",
    "                        os.remove(os.path.join(folder_path, img))\n",
    "\n",
    "    def split_dataset(self):\n",
    "            base_path = Path('artifacts/data_ingestion/bone_marrow_cell_dataset')  # Replace with your dataset path\n",
    "            train_dir = self.root_dir_path / 'train_data_set'\n",
    "            val_dir = self.root_dir_path / 'validation_data_set'\n",
    "\n",
    "            # Create train and validation directories if they don't exist\n",
    "            os.makedirs(train_dir, exist_ok=True)\n",
    "            os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "            for folder in os.listdir(base_path):\n",
    "                folder_path = base_path / folder\n",
    "\n",
    "                if os.path.isdir(folder_path):\n",
    "                    images = [img for img in os.listdir(folder_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                    random.shuffle(images)  # Shuffle images to randomize the split\n",
    "\n",
    "                    # Calculate split index\n",
    "                    split_index = int(len(images) * 0.75)\n",
    "\n",
    "                    # Split into training and validation images\n",
    "                    train_images = images[:split_index]\n",
    "                    val_images = images[split_index:]\n",
    "\n",
    "                    # Copy training images\n",
    "                    train_folder_path = train_dir / folder\n",
    "                    os.makedirs(train_folder_path, exist_ok=True)\n",
    "                    for img in train_images:\n",
    "                        shutil.copy(folder_path / img, train_folder_path / img)\n",
    "\n",
    "                    # Copy validation images\n",
    "                    val_folder_path = val_dir / folder\n",
    "                    os.makedirs(val_folder_path, exist_ok=True)\n",
    "                    for img in val_images:\n",
    "                        shutil.copy(folder_path / img, val_folder_path / img)    \n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-30 15:13:52,049:INFO:common:yaml file:config\\config.yaml loaded successfully]\n",
      "[2023-11-30 15:13:52,054:INFO:common:yaml file:params.yaml loaded successfully]\n",
      "[2023-11-30 15:13:52,059:INFO:common:directory:artifacts created successfully]\n",
      "[2023-11-30 15:13:52,064:INFO:common:directory:artifacts/data_ingestion created successfully]\n",
      "[2023-11-30 15:13:52,066:INFO:3335951008:Dataset already downloaded: artifacts/data_ingestion/data.zip]\n"
     ]
    }
   ],
   "source": [
    "# Main process\n",
    "root_dir = 'artifacts/data_ingestion/bone_marrow_cell_dataset'  # Update this path\n",
    "train_dir = 'artifacts/data_ingestion/train_data_set'  # Update this path\n",
    "val_dir = 'artifacts/data_ingestion/validation_data_set'\n",
    "# Main process\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(data_ingestion_config)\n",
    "\n",
    "    # Comment out or remove these lines if these steps are already done\n",
    "    data_ingestion.download_file()\n",
    "    #data_ingestion.extract_zip_file()\n",
    "    #data_ingestion.organize_dataset()\n",
    "    #data_ingestion.balance_dataset()\n",
    "\n",
    "    # This is the only method that will be executed\n",
    "    data_ingestion.split_dataset()\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data ingestion process: {e}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
